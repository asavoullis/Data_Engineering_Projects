{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Data Engineering Project\n",
    "\n",
    "### Using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data\n",
    "\n",
    "### Creating the Pyspark SQL Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>BO</th>\n",
       "      <th>BH</th>\n",
       "      <th>BL</th>\n",
       "      <th>BC</th>\n",
       "      <th>BCh</th>\n",
       "      <th>AO</th>\n",
       "      <th>AH</th>\n",
       "      <th>AL</th>\n",
       "      <th>AC</th>\n",
       "      <th>ACh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2005-05-02</td>\n",
       "      <td>00:00</td>\n",
       "      <td>1.2852</td>\n",
       "      <td>1.2852</td>\n",
       "      <td>1.2840</td>\n",
       "      <td>1.2844</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>1.2854</td>\n",
       "      <td>1.2854</td>\n",
       "      <td>1.2842</td>\n",
       "      <td>1.2846</td>\n",
       "      <td>-0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2005-05-02</td>\n",
       "      <td>01:00</td>\n",
       "      <td>1.2844</td>\n",
       "      <td>1.2848</td>\n",
       "      <td>1.2839</td>\n",
       "      <td>1.2842</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>1.2846</td>\n",
       "      <td>1.2850</td>\n",
       "      <td>1.2841</td>\n",
       "      <td>1.2844</td>\n",
       "      <td>-0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-05-02</td>\n",
       "      <td>02:00</td>\n",
       "      <td>1.2843</td>\n",
       "      <td>1.2854</td>\n",
       "      <td>1.2841</td>\n",
       "      <td>1.2851</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>1.2845</td>\n",
       "      <td>1.2856</td>\n",
       "      <td>1.2843</td>\n",
       "      <td>1.2853</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2005-05-02</td>\n",
       "      <td>03:00</td>\n",
       "      <td>1.2851</td>\n",
       "      <td>1.2859</td>\n",
       "      <td>1.2850</td>\n",
       "      <td>1.2851</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.2853</td>\n",
       "      <td>1.2861</td>\n",
       "      <td>1.2852</td>\n",
       "      <td>1.2853</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2005-05-02</td>\n",
       "      <td>04:00</td>\n",
       "      <td>1.2852</td>\n",
       "      <td>1.2859</td>\n",
       "      <td>1.2849</td>\n",
       "      <td>1.2855</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>1.2854</td>\n",
       "      <td>1.2861</td>\n",
       "      <td>1.2851</td>\n",
       "      <td>1.2857</td>\n",
       "      <td>0.0003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date   Time      BO      BH      BL      BC     BCh      AO      AH  \\\n",
       "0  2005-05-02  00:00  1.2852  1.2852  1.2840  1.2844 -0.0008  1.2854  1.2854   \n",
       "1  2005-05-02  01:00  1.2844  1.2848  1.2839  1.2842 -0.0002  1.2846  1.2850   \n",
       "2  2005-05-02  02:00  1.2843  1.2854  1.2841  1.2851  0.0008  1.2845  1.2856   \n",
       "3  2005-05-02  03:00  1.2851  1.2859  1.2850  1.2851  0.0000  1.2853  1.2861   \n",
       "4  2005-05-02  04:00  1.2852  1.2859  1.2849  1.2855  0.0003  1.2854  1.2861   \n",
       "\n",
       "       AL      AC     ACh  \n",
       "0  1.2842  1.2846 -0.0008  \n",
       "1  1.2841  1.2844 -0.0002  \n",
       "2  1.2843  1.2853  0.0008  \n",
       "3  1.2852  1.2853  0.0000  \n",
       "4  1.2851  1.2857  0.0003  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.read_csv('eurusd_hour.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(pd.read_csv('eurusd_hour.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "# data = [(1, \"Alice\", 25), (2, \"Bob\", 30), (3, \"Charlie\", 35)]\n",
    "# columns = [\"ID\", \"Name\", \"Age\"]\n",
    "\n",
    "# # Create a Spark session\n",
    "# spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# # Create a DataFrame\n",
    "# df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Project\").getOrCreate()\n",
    "# spark = SparkSession.builder.appName(\"Project\").config(\"spark.rpc.message.maxSize\", 2147483647).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-DBGGS1IF.Home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21735a80370>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('eurusd_hour.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|       _c0|  _c1|   _c2|   _c3|   _c4|   _c5|                 _c6|   _c7|   _c8|   _c9|  _c10|                _c11|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|      Date| Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "|2005-05-02|00:00|1.2852|1.2852| 1.284|1.2844|-0.00079999999999...|1.2854|1.2854|1.2842|1.2846|-0.00080000000000...|\n",
      "|2005-05-02|01:00|1.2844|1.2848|1.2839|1.2842|-0.00019999999999...|1.2846| 1.285|1.2841|1.2844|-0.00019999999999...|\n",
      "|2005-05-02|02:00|1.2843|1.2854|1.2841|1.2851|0.000799999999999...|1.2845|1.2856|1.2843|1.2853|0.000800000000000...|\n",
      "|2005-05-02|03:00|1.2851|1.2859| 1.285|1.2851|                 0.0|1.2853|1.2861|1.2852|1.2853|                 0.0|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: string, Time: string, BO: string, BH: string, BL: string, BC: string, BCh: string, AO: string, AH: string, AL: string, AC: string, ACh: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we do this to remove the extra columns names that pyspark has introduced\n",
    "spark.read.option('header','true').csv('eurusd_hour.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|      Date| Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|2005-05-02|00:00|1.2852|1.2852| 1.284|1.2844|-0.00079999999999...|1.2854|1.2854|1.2842|1.2846|-0.00080000000000...|\n",
      "|2005-05-02|01:00|1.2844|1.2848|1.2839|1.2842|-0.00019999999999...|1.2846| 1.285|1.2841|1.2844|-0.00019999999999...|\n",
      "|2005-05-02|02:00|1.2843|1.2854|1.2841|1.2851|0.000799999999999...|1.2845|1.2856|1.2843|1.2853|0.000800000000000...|\n",
      "|2005-05-02|03:00|1.2851|1.2859| 1.285|1.2851|                 0.0|1.2853|1.2861|1.2852|1.2853|                 0.0|\n",
      "|2005-05-02|04:00|1.2852|1.2859|1.2849|1.2855|0.000300000000000189|1.2854|1.2861|1.2851|1.2857|0.000299999999999...|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header','true').csv('eurusd_hour.csv').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('eurusd_hour.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)\n",
    "# this is a pyspark sql dataframe not a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|      Date| Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|2005-05-02|00:00|1.2852|1.2852| 1.284|1.2844|-0.00079999999999...|1.2854|1.2854|1.2842|1.2846|-0.00080000000000...|\n",
      "|2005-05-02|01:00|1.2844|1.2848|1.2839|1.2842|-0.00019999999999...|1.2846| 1.285|1.2841|1.2844|-0.00019999999999...|\n",
      "|2005-05-02|02:00|1.2843|1.2854|1.2841|1.2851|0.000799999999999...|1.2845|1.2856|1.2843|1.2853|0.000800000000000...|\n",
      "|2005-05-02|03:00|1.2851|1.2859| 1.285|1.2851|                 0.0|1.2853|1.2861|1.2852|1.2853|                 0.0|\n",
      "|2005-05-02|04:00|1.2852|1.2859|1.2849|1.2855|0.000300000000000189|1.2854|1.2861|1.2851|1.2857|0.000299999999999...|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- BO: string (nullable = true)\n",
      " |-- BH: string (nullable = true)\n",
      " |-- BL: string (nullable = true)\n",
      " |-- BC: string (nullable = true)\n",
      " |-- BCh: string (nullable = true)\n",
      " |-- AO: string (nullable = true)\n",
      " |-- AH: string (nullable = true)\n",
      " |-- AL: string (nullable = true)\n",
      " |-- AC: string (nullable = true)\n",
      " |-- ACh: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the Schema\n",
    "df_pyspark.printSchema()\n",
    "# same as df.info() - gives information about our columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all the columns are of type string.\n",
    "\n",
    "This is because pysparks will consider by default all the columns as strings unless we use a parameter called inferSchema\n",
    "\n",
    "So Let's correct this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|      Date|               Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "+----------+-------------------+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|2005-05-02|2023-09-28 00:00:00|1.2852|1.2852| 1.284|1.2844|-7.99999999999911...|1.2854|1.2854|1.2842|1.2846|-8.00000000000133...|\n",
      "|2005-05-02|2023-09-28 01:00:00|1.2844|1.2848|1.2839|1.2842|-1.99999999999978E-4|1.2846| 1.285|1.2841|1.2844|-1.99999999999978E-4|\n",
      "|2005-05-02|2023-09-28 02:00:00|1.2843|1.2854|1.2841|1.2851|7.999999999999119E-4|1.2845|1.2856|1.2843|1.2853|8.000000000001339E-4|\n",
      "|2005-05-02|2023-09-28 03:00:00|1.2851|1.2859| 1.285|1.2851|                 0.0|1.2853|1.2861|1.2852|1.2853|                 0.0|\n",
      "|2005-05-02|2023-09-28 04:00:00|1.2852|1.2859|1.2849|1.2855| 3.00000000000189E-4|1.2854|1.2861|1.2851|1.2857|2.999999999999669...|\n",
      "+----------+-------------------+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('eurusd_hour.csv', inferSchema = True)\n",
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- BO: double (nullable = true)\n",
      " |-- BH: double (nullable = true)\n",
      " |-- BL: double (nullable = true)\n",
      " |-- BC: double (nullable = true)\n",
      " |-- BCh: double (nullable = true)\n",
      " |-- AO: double (nullable = true)\n",
      " |-- AH: double (nullable = true)\n",
      " |-- AL: double (nullable = true)\n",
      " |-- AC: double (nullable = true)\n",
      " |-- ACh: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative Way\n",
    "\n",
    "But as we can see here an error was introduced in the Time COLUMN. \n",
    "\n",
    "A timestamp with the current date was introduced additionally to the time in the Time column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|      Date|               Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "+----------+-------------------+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|2005-05-02|2023-09-28 00:00:00|1.2852|1.2852| 1.284|1.2844|-7.99999999999911...|1.2854|1.2854|1.2842|1.2846|-8.00000000000133...|\n",
      "|2005-05-02|2023-09-28 01:00:00|1.2844|1.2848|1.2839|1.2842|-1.99999999999978E-4|1.2846| 1.285|1.2841|1.2844|-1.99999999999978E-4|\n",
      "|2005-05-02|2023-09-28 02:00:00|1.2843|1.2854|1.2841|1.2851|7.999999999999119E-4|1.2845|1.2856|1.2843|1.2853|8.000000000001339E-4|\n",
      "|2005-05-02|2023-09-28 03:00:00|1.2851|1.2859| 1.285|1.2851|                 0.0|1.2853|1.2861|1.2852|1.2853|                 0.0|\n",
      "|2005-05-02|2023-09-28 04:00:00|1.2852|1.2859|1.2849|1.2855| 3.00000000000189E-4|1.2854|1.2861|1.2851|1.2857|2.999999999999669...|\n",
      "+----------+-------------------+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('eurusd_hour.csv', header=True, inferSchema = True)\n",
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- BO: double (nullable = true)\n",
      " |-- BH: double (nullable = true)\n",
      " |-- BL: double (nullable = true)\n",
      " |-- BC: double (nullable = true)\n",
      " |-- BCh: double (nullable = true)\n",
      " |-- AO: double (nullable = true)\n",
      " |-- AH: double (nullable = true)\n",
      " |-- AL: double (nullable = true)\n",
      " |-- AC: double (nullable = true)\n",
      " |-- ACh: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# double checking the column types / schema of df\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|      Date| Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|2005-05-02|00:00|1.2852|1.2852| 1.284|1.2844|-7.99999999999911...|1.2854|1.2854|1.2842|1.2846|-8.00000000000133...|\n",
      "|2005-05-02|01:00|1.2844|1.2848|1.2839|1.2842|-1.99999999999978E-4|1.2846| 1.285|1.2841|1.2844|-1.99999999999978E-4|\n",
      "|2005-05-02|02:00|1.2843|1.2854|1.2841|1.2851|7.999999999999119E-4|1.2845|1.2856|1.2843|1.2853|8.000000000001339E-4|\n",
      "|2005-05-02|03:00|1.2851|1.2859| 1.285|1.2851|                 0.0|1.2853|1.2861|1.2852|1.2853|                 0.0|\n",
      "|2005-05-02|04:00|1.2852|1.2859|1.2849|1.2855| 3.00000000000189E-4|1.2854|1.2861|1.2851|1.2857|2.999999999999669...|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "df = df_pyspark.withColumn(\"Time\", date_format(\"Time\", \"HH:mm\"))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+---------------+\n",
      "|      Date| Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|       DateTime|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+---------------+\n",
      "|2005-05-02|00:00|1.2852|1.2852| 1.284|1.2844|-7.99999999999911...|1.2854|1.2854|1.2842|1.2846|-8.00000000000133...|2005-05-0200:00|\n",
      "|2005-05-02|01:00|1.2844|1.2848|1.2839|1.2842|-1.99999999999978E-4|1.2846| 1.285|1.2841|1.2844|-1.99999999999978E-4|2005-05-0201:00|\n",
      "|2005-05-02|02:00|1.2843|1.2854|1.2841|1.2851|7.999999999999119E-4|1.2845|1.2856|1.2843|1.2853|8.000000000001339E-4|2005-05-0202:00|\n",
      "|2005-05-02|03:00|1.2851|1.2859| 1.285|1.2851|                 0.0|1.2853|1.2861|1.2852|1.2853|                 0.0|2005-05-0203:00|\n",
      "|2005-05-02|04:00|1.2852|1.2859|1.2849|1.2855| 3.00000000000189E-4|1.2854|1.2861|1.2851|1.2857|2.999999999999669...|2005-05-0204:00|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "df = df.withColumn(\"DateTime\", concat(col(\"Date\"), col(\"Time\")))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+-------+-------+-------+--------------------+-------+-------+-------+-------+--------------------+-------------------+\n",
      "|      Date| Time|     BO|     BH|     BL|     BC|                 BCh|     AO|     AH|     AL|     AC|                 ACh|           DateTime|\n",
      "+----------+-----+-------+-------+-------+-------+--------------------+-------+-------+-------+-------+--------------------+-------------------+\n",
      "|2005-05-02|00:00| 1.2852| 1.2852|  1.284| 1.2844|-7.99999999999911...| 1.2854| 1.2854| 1.2842| 1.2846|-8.00000000000133...|2005-05-02 00:00:00|\n",
      "|2005-05-02|01:00| 1.2844| 1.2848| 1.2839| 1.2842|-1.99999999999978E-4| 1.2846|  1.285| 1.2841| 1.2844|-1.99999999999978E-4|2005-05-02 01:00:00|\n",
      "|2005-05-02|02:00| 1.2843| 1.2854| 1.2841| 1.2851|7.999999999999119E-4| 1.2845| 1.2856| 1.2843| 1.2853|8.000000000001339E-4|2005-05-02 02:00:00|\n",
      "|2005-05-02|03:00| 1.2851| 1.2859|  1.285| 1.2851|                 0.0| 1.2853| 1.2861| 1.2852| 1.2853|                 0.0|2005-05-02 03:00:00|\n",
      "|2005-05-02|04:00| 1.2852| 1.2859| 1.2849| 1.2855| 3.00000000000189E-4| 1.2854| 1.2861| 1.2851| 1.2857|2.999999999999669...|2005-05-02 04:00:00|\n",
      "|2005-05-02|05:00| 1.2854| 1.2858| 1.2853| 1.2854|                 0.0| 1.2856|  1.286| 1.2855| 1.2856|                 0.0|2005-05-02 05:00:00|\n",
      "|2005-05-02|06:00| 1.2854|  1.286| 1.2852|1.28585|4.499999999998393...| 1.2856| 1.2862| 1.2854|  1.286| 3.99999999999956E-4|2005-05-02 06:00:00|\n",
      "|2005-05-02|07:00|1.28585|1.28605|1.28515|1.28555|-2.99999999999966...|  1.286| 1.2862| 1.2853| 1.2857|-2.99999999999966...|2005-05-02 07:00:00|\n",
      "|2005-05-02|08:00|1.28555|1.28675|1.28555| 1.2864|8.500000000000174E-4| 1.2857| 1.2869| 1.2857|1.28655|8.500000000000174E-4|2005-05-02 08:00:00|\n",
      "|2005-05-02|09:00| 1.2864| 1.2868| 1.2862| 1.2868| 3.99999999999956E-4|1.28655|1.28695|1.28635|1.28695| 3.99999999999956E-4|2005-05-02 09:00:00|\n",
      "|2005-05-02|10:00| 1.2867| 1.2874| 1.2865|1.28715|4.500000000000615E-4|1.28685|1.28755|1.28665| 1.2873|4.500000000000615E-4|2005-05-02 10:00:00|\n",
      "|2005-05-02|11:00|1.28725|1.28735| 1.2858|  1.286|-0.00124999999999...| 1.2874| 1.2875|1.28595|1.28615|-0.00125000000000...|2005-05-02 11:00:00|\n",
      "|2005-05-02|12:00|  1.286| 1.2872| 1.2858| 1.2859| -9.9999999999989E-5|1.28615|1.28735|1.28595|1.28605| -9.9999999999989E-5|2005-05-02 12:00:00|\n",
      "|2005-05-02|13:00| 1.2858|1.28656| 1.2857| 1.2862| 3.99999999999956E-4|1.28595|  1.287|1.28585|  1.287|0.001049999999999...|2005-05-02 13:00:00|\n",
      "|2005-05-02|14:00| 1.2862| 1.2868|1.28367|1.28367|-0.00252999999999...|  1.287|  1.287|1.28387|1.28387|-0.00312999999999...|2005-05-02 14:00:00|\n",
      "|2005-05-02|15:00|1.28377|1.28587|1.28377|1.28567|0.001900000000000...|1.28397|1.28607|1.28397|1.28587|0.001900000000000...|2005-05-02 15:00:00|\n",
      "|2005-05-02|16:00|1.28557|1.28607| 1.2839|1.28442|-0.00114999999999...|1.28577|1.28627| 1.2841|1.28462|-0.00114999999999...|2005-05-02 16:00:00|\n",
      "|2005-05-02|17:00|1.28442|1.28512|1.28422|1.28502|5.999999999999339E-4|1.28462|1.28532|1.28442|1.28522|5.999999999999339E-4|2005-05-02 17:00:00|\n",
      "|2005-05-02|18:00|1.28502| 1.2862|1.28502|  1.286|9.799999999999807E-4|1.28522| 1.2864|1.28522| 1.2862|9.799999999999807E-4|2005-05-02 18:00:00|\n",
      "|2005-05-02|19:00|  1.286| 1.2862| 1.2856| 1.2858|-1.99999999999978E-4| 1.2862| 1.2864| 1.2858|  1.286|-1.99999999999978E-4|2005-05-02 19:00:00|\n",
      "|2005-05-02|20:00| 1.2858| 1.2864| 1.2857| 1.2863|4.999999999999449E-4|  1.286| 1.2866| 1.2859| 1.2865|4.999999999999449E-4|2005-05-02 20:00:00|\n",
      "|2005-05-02|21:00| 1.2863| 1.2864| 1.2851| 1.2851|-0.00120000000000009| 1.2865| 1.2866| 1.2853| 1.2853|-0.00119999999999...|2005-05-02 21:00:00|\n",
      "|2005-05-02|22:00| 1.2852| 1.2853| 1.2841| 1.2845|-6.99999999999922...| 1.2854| 1.2855| 1.2843| 1.2847|-7.00000000000145...|2005-05-02 22:00:00|\n",
      "|2005-05-02|23:00| 1.2844| 1.2858| 1.2844| 1.2854|0.001000000000000112| 1.2846|  1.286| 1.2846| 1.2856|0.001000000000000112|2005-05-02 23:00:00|\n",
      "|2005-05-03|00:00| 1.2855| 1.2856| 1.2849| 1.2852|-3.00000000000189E-4| 1.2857| 1.2858| 1.2851| 1.2854|-2.99999999999966...|2005-05-03 00:00:00|\n",
      "+----------+-----+-------+-------+-------+-------+--------------------+-------+-------+-------+-------+--------------------+-------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "df = df.withColumn(\"DateTime\", to_timestamp(col(\"DateTime\"), \"yyyy-MM-ddHH:mm\"))\n",
    "df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|           DateTime|      Date| Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|2005-05-02 00:00:00|2005-05-02|00:00|1.2852|1.2852| 1.284|1.2844|-7.99999999999911...|1.2854|1.2854|1.2842|1.2846|-8.00000000000133...|\n",
      "|2005-05-02 01:00:00|2005-05-02|01:00|1.2844|1.2848|1.2839|1.2842|-1.99999999999978E-4|1.2846| 1.285|1.2841|1.2844|-1.99999999999978E-4|\n",
      "|2005-05-02 02:00:00|2005-05-02|02:00|1.2843|1.2854|1.2841|1.2851|7.999999999999119E-4|1.2845|1.2856|1.2843|1.2853|8.000000000001339E-4|\n",
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# changing the order of the columns in the PySpark DataFrame\n",
    "df = df.select(\"DateTime\", \"Date\", \"Time\", \"BO\", \"BH\", \"BL\", \"BC\", \"BCh\", \"AO\", \"AH\", \"AL\", \"AC\", \"ACh\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DateTime: timestamp (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- BO: double (nullable = true)\n",
      " |-- BH: double (nullable = true)\n",
      " |-- BL: double (nullable = true)\n",
      " |-- BC: double (nullable = true)\n",
      " |-- BCh: double (nullable = true)\n",
      " |-- AO: double (nullable = true)\n",
      " |-- AH: double (nullable = true)\n",
      " |-- AL: double (nullable = true)\n",
      " |-- AC: double (nullable = true)\n",
      " |-- ACh: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the column types / schema of df\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column and Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "- **Date**: This column contains the date on which the financial data was recorded. Each row in the DataFrame corresponds to a specific date.\n",
    "\n",
    "- **Time**: This column provides the timestamp for each data point, indicating the exact time when the data was captured. This level of granularity allows for tracking changes throughout the trading day.\n",
    "\n",
    "- **Bid Open (BO)**: The \"Bid Open\" column represents the initial bid price at the start of a specific time interval. It's essentially the first price at which traders are willing to buy the EUR/USD currency pair during that interval.\n",
    "\n",
    "- **Bid High (BH)**: This column shows the highest bid price reached during the same time interval. It tells us the peak price that buyers were willing to pay within that period.\n",
    "\n",
    "- **Bid Low (BL)**: Here, we have the lowest bid price observed during the time interval, indicating the lowest price at which traders were willing to buy the currency pair.\n",
    "\n",
    "- **Bid Close (BC)**: The closing bid price signifies the last recorded bid price at the end of the time interval, which can provide insights into how trading sentiment may have evolved during that period.\n",
    "\n",
    "- **Bid Change (BCh)**: This column seems to represent the change in bid price during the interval, possibly calculated as the difference between the opening and closing bid prices. It helps track price movements.\n",
    "\n",
    "- **Ask Open (AO)**: Similar to the bid open, this is the initial asking price for the EUR/USD currency pair at the beginning of the time interval. It represents the first price at which sellers are willing to sell.\n",
    "\n",
    "- **Ask High (AH)**: The highest asking price during the time interval is found here. It indicates the peak price sellers were requesting for the currency pair.\n",
    "\n",
    "- **Ask Low (AL)**: The lowest asking price observed during the interval is listed in this column, representing the lowest price sellers were willing to accept.\n",
    "\n",
    "- **Ask Close (AC)**: The closing asking price signifies the last recorded asking price at the end of the time interval. Like the closing bid price, it offers insights into potential shifts in market sentiment.\n",
    "\n",
    "- **Ask Change (ACh)**: Similar to the bid change, this column appears to represent the change in asking price during the interval, possibly calculated as the difference between the opening and closing asking prices. It helps monitor price fluctuations from the perspective of sellers.\n",
    "\n",
    "This dataset provides a comprehensive view of trading activity for the EUR/USD currency pair, including open, high, low, and close prices for both bid and ask prices. It's invaluable for financial analysis and can be used to analyze market trends, make trading decisions, and gain insights into the behavior of currency pairs in the foreign exchange market.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the dimensions of our Pyspark SQL Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|           DateTime|      Date| Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|2005-05-02 00:00:00|2005-05-02|00:00|1.2852|1.2852| 1.284|1.2844|-7.99999999999911...|1.2854|1.2854|1.2842|1.2846|-8.00000000000133...|\n",
      "|2005-05-02 01:00:00|2005-05-02|01:00|1.2844|1.2848|1.2839|1.2842|-1.99999999999978E-4|1.2846| 1.285|1.2841|1.2844|-1.99999999999978E-4|\n",
      "|2005-05-02 02:00:00|2005-05-02|02:00|1.2843|1.2854|1.2841|1.2851|7.999999999999119E-4|1.2845|1.2856|1.2843|1.2853|8.000000000001339E-4|\n",
      "|2005-05-02 03:00:00|2005-05-02|03:00|1.2851|1.2859| 1.285|1.2851|                 0.0|1.2853|1.2861|1.2852|1.2853|                 0.0|\n",
      "|2005-05-02 04:00:00|2005-05-02|04:00|1.2852|1.2859|1.2849|1.2855| 3.00000000000189E-4|1.2854|1.2861|1.2851|1.2857|2.999999999999669...|\n",
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DateTime',\n",
       " 'Date',\n",
       " 'Time',\n",
       " 'BO',\n",
       " 'BH',\n",
       " 'BL',\n",
       " 'BC',\n",
       " 'BCh',\n",
       " 'AO',\n",
       " 'AH',\n",
       " 'AL',\n",
       " 'AC',\n",
       " 'ACh']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = df.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 93084\n",
      "Number of Columns: 13\n"
     ]
    }
   ],
   "source": [
    "# and 13 columns ( 1 that I introduced - DateTime )\n",
    "# we have 93084 rows in our dataframe\n",
    "print(f\"Number of Rows: {df.count()}\")\n",
    "print(f\"Number of Columns: {len(columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93084"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting Specific Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Date'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this doesn't work  like pandas\n",
    "df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: date]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this doesn't work either\n",
    "df.select(['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2005-05-02|\n",
      "|2005-05-02|\n",
      "|2005-05-02|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['Date']).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+------+\n",
      "|           Datetime|      Date| Time|    BO|\n",
      "+-------------------+----------+-----+------+\n",
      "|2005-05-02 00:00:00|2005-05-02|00:00|1.2852|\n",
      "|2005-05-02 01:00:00|2005-05-02|01:00|1.2844|\n",
      "|2005-05-02 02:00:00|2005-05-02|02:00|1.2843|\n",
      "+-------------------+----------+-----+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can also selected multiple columns\n",
    "df.select(['Datetime','Date','Time','BO']).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Datetime=datetime.datetime(2005, 5, 2, 0, 0), Date=datetime.date(2005, 5, 2), Time='00:00', BO=1.2852),\n",
       " Row(Datetime=datetime.datetime(2005, 5, 2, 1, 0), Date=datetime.date(2005, 5, 2), Time='01:00', BO=1.2844),\n",
       " Row(Datetime=datetime.datetime(2005, 5, 2, 2, 0), Date=datetime.date(2005, 5, 2), Time='02:00', BO=1.2843)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(['Datetime','Date','Time','BO']).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2005, 5, 2)),\n",
       " Row(Date=datetime.date(2005, 5, 2)),\n",
       " Row(Date=datetime.date(2005, 5, 2))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select(['Date']).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DateTime', 'timestamp'),\n",
       " ('Date', 'date'),\n",
       " ('Time', 'string'),\n",
       " ('BO', 'double'),\n",
       " ('BH', 'double'),\n",
       " ('BL', 'double'),\n",
       " ('BC', 'double'),\n",
       " ('BCh', 'double'),\n",
       " ('AO', 'double'),\n",
       " ('AH', 'double'),\n",
       " ('AL', 'double'),\n",
       " ('AC', 'double'),\n",
       " ('ACh', 'double')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the data types of columns in the DataFrame.\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Time: string, BO: string, BH: string, BL: string, BC: string, BCh: string, AO: string, AH: string, AL: string, AC: string, ACh: string]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate summary statistics for each numerical column in the DataFrame.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apparently we have 6 days of week worth of data in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2005-05-02|\n",
      "|2005-05-03|\n",
      "|2005-05-04|\n",
      "|2005-05-05|\n",
      "|2005-05-06|\n",
      "|2005-05-08|\n",
      "|2005-05-09|\n",
      "|2005-05-10|\n",
      "|2005-05-11|\n",
      "|2005-05-12|\n",
      "|2005-05-13|\n",
      "|2005-05-15|\n",
      "|2005-05-16|\n",
      "|2005-05-17|\n",
      "|2005-05-18|\n",
      "|2005-05-19|\n",
      "|2005-05-20|\n",
      "|2005-05-22|\n",
      "|2005-05-23|\n",
      "|2005-05-24|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "sorted_dates = df_pyspark.select('Date').distinct().orderBy(col('Date'))\n",
    "\n",
    "sorted_dates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out why we have data for 6 days a week instead of 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forex (foreign exchange) markets are generally open and available for trading 24 hours a day, five days a week. The forex market operates globally and is divided into different trading sessions, which include:\n",
    "\n",
    "Sydney Session: This session starts at 10:00 PM GMT and ends at 7:00 AM GMT. It is the first major session to open.\n",
    "\n",
    "Tokyo Session: Following the Sydney session, the Tokyo session begins at 11:00 PM GMT and ends at 8:00 AM GMT.\n",
    "\n",
    "London Session: The London session is one of the most significant and active trading sessions. It starts at 8:00 AM GMT and ends at 4:00 PM GMT.\n",
    "\n",
    "New York Session: The New York session overlaps with the London session and is open from 1:00 PM GMT to 10:00 PM GMT. This overlap period is typically the busiest time for forex trading.\n",
    "\n",
    "During these trading sessions, currency pairs like EUR/USD (Euro/US Dollar) are actively traded. However, it's important to note that the forex market is closed on weekends (from Friday evening until Sunday evening GMT) and during certain holidays when major financial centers are closed.\n",
    "\n",
    "So apparently forex trading is generally available every day from Sunday evening GMT to Friday evening GMT,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|           DateTime|      Date| Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|2005-05-02 00:00:00|2005-05-02|00:00|1.2852|1.2852| 1.284|1.2844|-7.99999999999911...|1.2854|1.2854|1.2842|1.2846|-8.00000000000133...|\n",
      "|2005-05-02 01:00:00|2005-05-02|01:00|1.2844|1.2848|1.2839|1.2842|-1.99999999999978E-4|1.2846| 1.285|1.2841|1.2844|-1.99999999999978E-4|\n",
      "|2005-05-02 02:00:00|2005-05-02|02:00|1.2843|1.2854|1.2841|1.2851|7.999999999999119E-4|1.2845|1.2856|1.2843|1.2853|8.000000000001339E-4|\n",
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+-------+-------+-------+-------+--------------------+------------------+-------+-------+-------+--------------------+\n",
      "|           DateTime|      Date| Time|     BO|     BH|     BL|     BC|                 BCh|                AO|     AH|     AL|     AC|                 ACh|\n",
      "+-------------------+----------+-----+-------+-------+-------+-------+--------------------+------------------+-------+-------+-------+--------------------+\n",
      "|2019-09-15 21:00:00|2019-09-15|21:00|1.10727|1.10743|1.10678|1.10712|-1.49999999999872...|           1.10768| 1.1079| 1.1072|1.10749|-1.90000000000134...|\n",
      "|2019-09-15 22:00:00|2019-09-15|22:00|1.10712|1.10794|1.10665|1.10785|7.299999999998974E-4|           1.10755|1.10811|1.10734|1.10801|4.599999999999049E-4|\n",
      "|2019-09-15 23:00:00|2019-09-15|23:00|1.10786|1.10858|1.10783|1.10825|3.899999999998904E-4|1.1080299999999998|1.10871|1.10798|1.10839| 3.60000000000138E-4|\n",
      "+-------------------+----------+-----+-------+-------+-------+-------+--------------------+------------------+-------+-------+-------+--------------------+\n",
      "\n",
      "The selected date (2019-09-15) was a Sunday.\n",
      "Hour range for 2019-09-15:         21:00      to      23:00\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "selected_date = \"2019-09-15\"\n",
    "\n",
    "# Filter the DataFrame to select data for the specified date (we picked a random Sunday)\n",
    "filtered_df = df.filter(df['Date'] == selected_date)\n",
    "\n",
    "# Show the filtered data\n",
    "filtered_df.show()\n",
    "\n",
    "hour_range = filtered_df.select(min('Time'), max('Time')).first()\n",
    "\n",
    "# Extract the minimum and maximum values\n",
    "min_hour = hour_range[0]\n",
    "max_hour = hour_range[1]\n",
    "\n",
    "day_of_week = filtered_df.select(date_format('Date', 'EEEE').alias('DayOfWeek')).first()\n",
    "\n",
    "# Extract the day of the week\n",
    "day_of_week = day_of_week['DayOfWeek']\n",
    "\n",
    "# Display the day of the week\n",
    "print(f\"The selected date ({selected_date}) was a {day_of_week}.\")\n",
    "\n",
    "# Display the hour range\n",
    "print(f\"Hour range for {selected_date}:         {min_hour}      to      {max_hour}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+------------------+------------------+------------------+------------------+--------------------+------------------+-------+------------------+------------------+--------------------+\n",
      "|           DateTime|      Date| Time|                BO|                BH|                BL|                BC|                 BCh|                AO|     AH|                AL|                AC|                 ACh|\n",
      "+-------------------+----------+-----+------------------+------------------+------------------+------------------+--------------------+------------------+-------+------------------+------------------+--------------------+\n",
      "|2019-09-13 00:00:00|2019-09-13|00:00|1.1055700000000002|           1.10643|1.1055700000000002|           1.10636|7.899999999998464E-4|           1.10573|1.10656|           1.10572|           1.10649|7.600000000000938E-4|\n",
      "|2019-09-13 01:00:00|2019-09-13|01:00|           1.10635|           1.10658|           1.10622|           1.10641|6.000000000017103...|           1.10648| 1.1067|           1.10636|           1.10654|5.999999999994897...|\n",
      "|2019-09-13 02:00:00|2019-09-13|02:00|            1.1064|           1.10659|           1.10633|1.1065200000000002|  1.2000000000012E-4|           1.10653|1.10672|           1.10647|           1.10665|1.199999999998979...|\n",
      "|2019-09-13 03:00:00|2019-09-13|03:00|           1.10653|           1.10687|            1.1065|           1.10686|3.299999999999414...|           1.10666|  1.107|           1.10663|             1.107|3.400000000000069...|\n",
      "|2019-09-13 04:00:00|2019-09-13|04:00|           1.10685|           1.10727|           1.10663|           1.10689|4.000000000004000...|           1.10699|1.10741|           1.10676|           1.10702|3.000000000019653...|\n",
      "|2019-09-13 05:00:00|2019-09-13|05:00|           1.10687|           1.10725|           1.10686|           1.10703|1.599999999999379...|           1.10701|1.10739|           1.10698|           1.10717|1.599999999999379...|\n",
      "|2019-09-13 06:00:00|2019-09-13|06:00|           1.10705|           1.10961|           1.10673|           1.10952|0.002469999999999972|1.1071799999999998|1.10973|           1.10686|           1.10965|0.002470000000000...|\n",
      "|2019-09-13 07:00:00|2019-09-13|07:00|           1.10953|            1.1109|           1.10912|           1.10934|-1.89999999999912...|           1.10966|1.11102|           1.10925|           1.10945|-2.10000000000043...|\n",
      "|2019-09-13 08:00:00|2019-09-13|08:00|           1.10932|           1.10971|           1.10765|           1.10917|-1.50000000000094...|           1.10945|1.10983|           1.10781|           1.10932|-1.29999999999963...|\n",
      "|2019-09-13 09:00:00|2019-09-13|09:00|           1.10919|           1.11038|            1.1089|           1.11002|8.300000000001084E-4|           1.10932| 1.1105|           1.10904|           1.11012|7.999999999999119E-4|\n",
      "|2019-09-13 10:00:00|2019-09-13|10:00|           1.11002|           1.11056|           1.10899|           1.10909|-9.30000000000097...|           1.11014|1.11068|           1.10912|           1.10921|-9.29999999999875...|\n",
      "|2019-09-13 11:00:00|2019-09-13|11:00|1.1090799999999998|           1.11061|           1.10904|           1.11022|0.001140000000000...|            1.1092|1.11073|           1.10917|           1.11034|0.001139999999999...|\n",
      "|2019-09-13 12:00:00|2019-09-13|12:00|           1.11021|           1.11044|           1.10785|            1.1087|-0.00151000000000...|1.1103299999999998|1.11057|           1.10798|           1.10883|-0.00149999999999...|\n",
      "|2019-09-13 13:00:00|2019-09-13|13:00|           1.10869|           1.10876|            1.1065|           1.10682|-0.00186999999999...|           1.10882|1.10889|1.1066200000000002|           1.10697|-0.00185000000000...|\n",
      "|2019-09-13 14:00:00|2019-09-13|14:00|           1.10679|1.1085200000000002|1.1060299999999998|           1.10799| 0.00120000000000009|           1.10696|1.10864|           1.10615|           1.10811|0.001149999999999...|\n",
      "|2019-09-13 15:00:00|2019-09-13|15:00|             1.108|            1.1085|           1.10689|           1.10735|-6.49999999999817...|           1.10812|1.10861|           1.10701|1.1074700000000002|-6.49999999999817...|\n",
      "|2019-09-13 16:00:00|2019-09-13|16:00|           1.10733|           1.10761|1.1066200000000002|           1.10754|2.100000000000434...|1.1074700000000002|1.10774|           1.10674|           1.10765|1.799999999998469...|\n",
      "|2019-09-13 17:00:00|2019-09-13|17:00|           1.10753|           1.10793|             1.107|           1.10759|6.000000000017103...|           1.10764|1.10806|           1.10712|           1.10771|7.000000000001451E-5|\n",
      "|2019-09-13 18:00:00|2019-09-13|18:00|           1.10758|1.1076700000000002|1.1066200000000002|           1.10665|-9.30000000000097...|            1.1077|1.10779|           1.10676|           1.10678|-9.20000000000032E-4|\n",
      "|2019-09-13 19:00:00|2019-09-13|19:00|           1.10666|1.1076700000000002|           1.10666|           1.10759|9.300000000000974E-4|           1.10679| 1.1078|           1.10679|           1.10772|9.300000000000974E-4|\n",
      "+-------------------+----------+-----+------------------+------------------+------------------+------------------+--------------------+------------------+-------+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "The selected date (2019-09-13) was a Friday.\n",
      "Hour range for 2019-09-13:         00:00      to      20:00\n"
     ]
    }
   ],
   "source": [
    "selected_date = \"2019-09-13\"\n",
    "\n",
    "\n",
    "# Filter the DataFrame to select data for the specified date (we picked a random Sunday)\n",
    "filtered_df = df.filter(df['Date'] == selected_date)\n",
    "\n",
    "# Show the filtered data\n",
    "filtered_df.show()\n",
    "\n",
    "hour_range = filtered_df.select(min('Time'), max('Time')).first()\n",
    "\n",
    "# Extract the minimum and maximum values\n",
    "min_hour = hour_range[0]\n",
    "max_hour = hour_range[1]\n",
    "\n",
    "day_of_week = filtered_df.select(date_format('Date', 'EEEE').alias('DayOfWeek')).first()\n",
    "\n",
    "# Extract the day of the week\n",
    "day_of_week = day_of_week['DayOfWeek']\n",
    "\n",
    "# Display the day of the week\n",
    "print(f\"The selected date ({selected_date}) was a {day_of_week}.\")\n",
    "\n",
    "# Display the hour range\n",
    "print(f\"Hour range for {selected_date}:         {min_hour}      to      {max_hour}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+------------------+------------------+-------+------------------+--------------------+------------------+------------------+------------------+------------------+--------------------+\n",
      "|           DateTime|      Date| Time|                BO|                BH|     BL|                BC|                 BCh|                AO|                AH|                AL|                AC|                 ACh|\n",
      "+-------------------+----------+-----+------------------+------------------+-------+------------------+--------------------+------------------+------------------+------------------+------------------+--------------------+\n",
      "|2019-09-12 00:00:00|2019-09-12|00:00|           1.10131|1.1013700000000002|1.10074|             1.101|-3.10000000000032...|           1.10144|           1.10149|           1.10086|           1.10114|-2.99999999999966...|\n",
      "|2019-09-12 01:00:00|2019-09-12|01:00|           1.10102|1.1014700000000002|1.10102|           1.10131|2.899999999999014E-4|           1.10114|            1.1016|           1.10114|           1.10143|2.899999999999014E-4|\n",
      "|2019-09-12 02:00:00|2019-09-12|02:00|           1.10128|           1.10129|1.10081|            1.1009|-3.80000000000046...|           1.10142|           1.10142|           1.10095|           1.10104|-3.80000000000046...|\n",
      "|2019-09-12 03:00:00|2019-09-12|03:00|           1.10092|            1.1013|1.10092|           1.10124| 3.19999999999876E-4|           1.10105|           1.10143|           1.10105|           1.10138|3.299999999999414...|\n",
      "|2019-09-12 04:00:00|2019-09-12|04:00|           1.10123|           1.10142|1.10113|           1.10135|  1.2000000000012E-4|1.1013700000000002|           1.10155|           1.10126|1.1014700000000002|  9.9999999999989E-5|\n",
      "|2019-09-12 05:00:00|2019-09-12|05:00|           1.10133|           1.10144|1.10115|           1.10129|-4.00000000000400...|           1.10146|           1.10156|           1.10127|           1.10142|-4.00000000000400...|\n",
      "|2019-09-12 06:00:00|2019-09-12|06:00|           1.10129|           1.10143|1.10051|1.1013700000000002|8.000000000030205E-5|           1.10142|           1.10155|1.1006200000000002|            1.1015|7.999999999985796E-5|\n",
      "|2019-09-12 07:00:00|2019-09-12|07:00|1.1013700000000002|           1.10184| 1.1011|           1.10164|2.699999999997704E-4|           1.10148|           1.10197|           1.10123|           1.10177|2.900000000001235E-4|\n",
      "|2019-09-12 08:00:00|2019-09-12|08:00|           1.10166|           1.10319|1.10151|           1.10264|9.799999999999807E-4|           1.10177|           1.10332|           1.10163|           1.10278|0.001009999999999...|\n",
      "|2019-09-12 09:00:00|2019-09-12|09:00|           1.10263|           1.10309|1.10244|            1.1029|2.699999999999924...|           1.10277|           1.10322|           1.10258|           1.10302|2.500000000000835E-4|\n",
      "|2019-09-12 10:00:00|2019-09-12|10:00|            1.1029|           1.10312|1.10192|           1.10233|-5.69999999999959...|           1.10304|           1.10323|1.1020299999999998|           1.10247|-5.69999999999959...|\n",
      "|2019-09-12 11:00:00|2019-09-12|11:00|           1.10236|           1.10663|1.09604|1.0965200000000002|-0.00583999999999...|           1.10247|           1.10743|           1.09619|           1.09666|-0.00581000000000...|\n",
      "|2019-09-12 12:00:00|2019-09-12|12:00|           1.09651|           1.09728|1.09262|           1.09454|-0.00197000000000...|           1.09665|           1.09741|           1.09275|           1.09467|-0.00197999999999...|\n",
      "|2019-09-12 13:00:00|2019-09-12|13:00|           1.09451|            1.1046|1.09397|1.1034700000000002|0.008960000000000079|           1.09466|           1.10477|           1.09411|            1.1036|0.008939999999999948|\n",
      "|2019-09-12 14:00:00|2019-09-12|14:00|           1.10348|           1.10686|1.10279|            1.1029|-5.80000000000024...|           1.10361|           1.10699|           1.10293|           1.10303|-5.80000000000024...|\n",
      "|2019-09-12 15:00:00|2019-09-12|15:00|           1.10292|           1.10789|1.10255|            1.1073|0.004379999999999...|           1.10304|1.1080299999999998|           1.10268|           1.10741|0.004369999999999985|\n",
      "|2019-09-12 16:00:00|2019-09-12|16:00|           1.10729|           1.10739| 1.1061|           1.10672|-5.69999999999737...|            1.1074|           1.10751|1.1062299999999998|           1.10685|-5.50000000000050...|\n",
      "|2019-09-12 17:00:00|2019-09-12|17:00|           1.10671|           1.10866|1.10654|           1.10665|-6.00000000001710...|           1.10683|            1.1088|           1.10666|           1.10677|-5.99999999999489...|\n",
      "|2019-09-12 18:00:00|2019-09-12|18:00|           1.10666|           1.10748|1.10632|           1.10734|6.800000000000138E-4|           1.10678|           1.10759|           1.10643|           1.10745|6.700000000001705E-4|\n",
      "|2019-09-12 19:00:00|2019-09-12|19:00|           1.10734|           1.10734|1.10618|           1.10645|-8.90000000000057...|1.1074700000000002|1.1074700000000002|            1.1063|           1.10655|-9.20000000000254E-4|\n",
      "+-------------------+----------+-----+------------------+------------------+-------+------------------+--------------------+------------------+------------------+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "The selected date (2019-09-12) was a Thursday.\n",
      "Hour range for 2019-09-12:         00:00      to      23:00\n"
     ]
    }
   ],
   "source": [
    "selected_date = \"2019-09-12\"\n",
    "\n",
    "# Filter the DataFrame to select data for the specified date (we picked a random Sunday)\n",
    "filtered_df = df.filter(df['Date'] == selected_date)\n",
    "\n",
    "# Show the filtered data\n",
    "filtered_df.show()\n",
    "\n",
    "hour_range = filtered_df.select(min('Time'), max('Time')).first()\n",
    "\n",
    "# Extract the minimum and maximum values\n",
    "min_hour = hour_range[0]\n",
    "max_hour = hour_range[1]\n",
    "\n",
    "day_of_week = filtered_df.select(date_format('Date', 'EEEE').alias('DayOfWeek')).first()\n",
    "\n",
    "# Extract the day of the week\n",
    "day_of_week = day_of_week['DayOfWeek']\n",
    "\n",
    "# Display the day of the week\n",
    "print(f\"The selected date ({selected_date}) was a {day_of_week}.\")\n",
    "\n",
    "# Display the hour range\n",
    "print(f\"Hour range for {selected_date}:         {min_hour}      to      {max_hour}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So apparently the Data Range for each week is from :\n",
    "\n",
    "Sunday              from 21:00 to 23:00\n",
    "\n",
    "Monday - Thursday   from 00:00 to 23:00\n",
    "\n",
    "Friday              from 00:00 to 21:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the summary statistics generated in the previous step in a tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "|summary| Time|                 BO|                BH|                BL|                BC|                 BCh|                 AO|                AH|                AL|                AC|                 ACh|\n",
      "+-------+-----+-------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "|  count|93084|              93084|             93084|             93084|             93084|               93084|              93084|             93084|             93084|             93084|               93084|\n",
      "|   mean| NULL| 1.2657340146534282|1.2667553338919628|1.2647362568217932| 1.265733121803963|-8.92849469295910...| 1.2659002723346635|1.2669171722315304|1.2648967919298715|1.2658992623866556|-1.00994800395440...|\n",
      "| stddev| NULL|0.12689441842290416|0.1270596992463785| 0.126722172862712|0.1268923255197565|0.001544142859950...|0.12689052960293262|0.1270531364216383|0.1267152261267547|0.1268911503779533|0.001545720757460...|\n",
      "|    min|00:00| 1.0355299999999998|1.0369700000000002|           1.03395|           1.03555|-0.02505000000000...|            1.03566|            1.0371|           1.03409|           1.03568|-0.02495000000000...|\n",
      "|    max|23:00|            1.60131|           1.60384|            1.5992|           1.60139|0.030219999999999917|             1.6014|           1.60393|           1.59929|           1.60148|0.030429999999999964|\n",
      "+-------+-----+-------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find the null values in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+---+---+---+---+---+---+---+---+---+---+\n",
      "|DateTime|Date|Time| BO| BH| BL| BC|BCh| AO| AH| AL| AC|ACh|\n",
      "+--------+----+----+---+---+---+---+---+---+---+---+---+---+\n",
      "|       0|   0|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+--------+----+----+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts = df.select([sum(col(column).isNull().cast(\"int\")).alias(column) for column in df.columns])\n",
    "\n",
    "# Show the result\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when how is set to all , then only if the entire row has NULL values it will be dropped\n",
    "df = df.na.drop(how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93084"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we drop the row if it doesn't have 2 NON NULL values in the cells of the row\n",
    "df = df.na.drop(how=\"all\", thresh=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93084"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's nice! we have 0 null values in our DataFrame!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Column Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating New Columns and Explain Why\n",
    "# Let's assume you want to calculate the daily price range as a new column\n",
    "# Price range = High Price (BH) - Low Price (BL)\n",
    "\n",
    "# df = df.withColumn(\"PriceRange\", col(\"BH\") - col(\"BL\"))\n",
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
