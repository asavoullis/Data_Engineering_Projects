{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Data Engineering Project\n",
    "\n",
    "### Using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data\n",
    "\n",
    "### Creating the Pyspark SQL Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('eurusd_hour.csv').head()\n",
    "# type(pd.read_csv('DataSet/eurusd_hour.csv'))\n",
    "\n",
    "# Sample DataFrame\n",
    "# data = [(1, \"Alice\", 25), (2, \"Bob\", 30), (3, \"Charlie\", 35)]\n",
    "# columns = [\"ID\", \"Name\", \"Age\"]\n",
    "\n",
    "# # Create a Spark session\n",
    "# spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# # Create a DataFrame\n",
    "# df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Project\").getOrCreate()\n",
    "# spark = SparkSession.builder.appName(\"Project\").config(\"spark.rpc.message.maxSize\", 2147483647).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-DBGGS1IF.Home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b329312700>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|      Date| Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|2005-05-02|00:00|1.2852|1.2852| 1.284|1.2844|-0.00079999999999...|1.2854|1.2854|1.2842|1.2846|-0.00080000000000...|\n",
      "|2005-05-02|01:00|1.2844|1.2848|1.2839|1.2842|-0.00019999999999...|1.2846| 1.285|1.2841|1.2844|-0.00019999999999...|\n",
      "|2005-05-02|02:00|1.2843|1.2854|1.2841|1.2851|0.000799999999999...|1.2845|1.2856|1.2843|1.2853|0.000800000000000...|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw = spark.read.option('header','true').csv('DataSet/eurusd_hour.csv')\n",
    "df_raw.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: date, Time: timestamp, BO: double, BH: double, BL: double, BC: double, BCh: double, AO: double, AH: double, AL: double, AC: double, ACh: double]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('DataSet/eurusd_hour.csv', inferSchema = True)\n",
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|      Date|               Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "+----------+-------------------+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|2005-05-02|2023-10-03 00:00:00|1.2852|1.2852| 1.284|1.2844|-7.99999999999911...|1.2854|1.2854|1.2842|1.2846|-8.00000000000133...|\n",
      "|2005-05-02|2023-10-03 01:00:00|1.2844|1.2848|1.2839|1.2842|-1.99999999999978E-4|1.2846| 1.285|1.2841|1.2844|-1.99999999999978E-4|\n",
      "|2005-05-02|2023-10-03 02:00:00|1.2843|1.2854|1.2841|1.2851|7.999999999999119E-4|1.2845|1.2856|1.2843|1.2853|8.000000000001339E-4|\n",
      "|2005-05-02|2023-10-03 03:00:00|1.2851|1.2859| 1.285|1.2851|                 0.0|1.2853|1.2861|1.2852|1.2853|                 0.0|\n",
      "|2005-05-02|2023-10-03 04:00:00|1.2852|1.2859|1.2849|1.2855| 3.00000000000189E-4|1.2854|1.2861|1.2851|1.2857|2.999999999999669...|\n",
      "+----------+-------------------+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)\n",
    "# this is a pyspark sql dataframe not a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- BO: double (nullable = true)\n",
      " |-- BH: double (nullable = true)\n",
      " |-- BL: double (nullable = true)\n",
      " |-- BC: double (nullable = true)\n",
      " |-- BCh: double (nullable = true)\n",
      " |-- AO: double (nullable = true)\n",
      " |-- AH: double (nullable = true)\n",
      " |-- AL: double (nullable = true)\n",
      " |-- AC: double (nullable = true)\n",
      " |-- ACh: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the Schema\n",
    "df_pyspark.printSchema()\n",
    "# same as df.info() - gives information about our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'date'),\n",
       " ('Time', 'timestamp'),\n",
       " ('BO', 'double'),\n",
       " ('BH', 'double'),\n",
       " ('BL', 'double'),\n",
       " ('BC', 'double'),\n",
       " ('BCh', 'double'),\n",
       " ('AO', 'double'),\n",
       " ('AH', 'double'),\n",
       " ('AL', 'double'),\n",
       " ('AC', 'double'),\n",
       " ('ACh', 'double')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double checking the column types / schema of df\n",
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|      Date|               Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "+----------+-------------------+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|2005-05-02|2023-10-03 00:00:00|1.2852|1.2852| 1.284|1.2844|-7.99999999999911...|1.2854|1.2854|1.2842|1.2846|-8.00000000000133...|\n",
      "|2005-05-02|2023-10-03 01:00:00|1.2844|1.2848|1.2839|1.2842|-1.99999999999978E-4|1.2846| 1.285|1.2841|1.2844|-1.99999999999978E-4|\n",
      "|2005-05-02|2023-10-03 02:00:00|1.2843|1.2854|1.2841|1.2851|7.999999999999119E-4|1.2845|1.2856|1.2843|1.2853|8.000000000001339E-4|\n",
      "+----------+-------------------+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here an error was introduced in the Time COLUMN. \n",
    "\n",
    "A timestamp with the current date was introduced additionally to the time in the Time column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|      Date| Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|2005-05-02|00:00|1.2852|1.2852| 1.284|1.2844|-7.99999999999911...|1.2854|1.2854|1.2842|1.2846|-8.00000000000133...|\n",
      "|2005-05-02|01:00|1.2844|1.2848|1.2839|1.2842|-1.99999999999978E-4|1.2846| 1.285|1.2841|1.2844|-1.99999999999978E-4|\n",
      "|2005-05-02|02:00|1.2843|1.2854|1.2841|1.2851|7.999999999999119E-4|1.2845|1.2856|1.2843|1.2853|8.000000000001339E-4|\n",
      "|2005-05-02|03:00|1.2851|1.2859| 1.285|1.2851|                 0.0|1.2853|1.2861|1.2852|1.2853|                 0.0|\n",
      "|2005-05-02|04:00|1.2852|1.2859|1.2849|1.2855| 3.00000000000189E-4|1.2854|1.2861|1.2851|1.2857|2.999999999999669...|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# keeping only the hours and minutes of the time column\n",
    "df = df_pyspark.withColumn(\"Time\", date_format(\"Time\", \"HH:mm\"))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- BO: double (nullable = true)\n",
      " |-- BH: double (nullable = true)\n",
      " |-- BL: double (nullable = true)\n",
      " |-- BC: double (nullable = true)\n",
      " |-- BCh: double (nullable = true)\n",
      " |-- AO: double (nullable = true)\n",
      " |-- AH: double (nullable = true)\n",
      " |-- AL: double (nullable = true)\n",
      " |-- AC: double (nullable = true)\n",
      " |-- ACh: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of the time column has become string now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+---------------+\n",
      "|      Date| Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|       DateTime|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+---------------+\n",
      "|2005-05-02|00:00|1.2852|1.2852| 1.284|1.2844|-7.99999999999911...|1.2854|1.2854|1.2842|1.2846|-8.00000000000133...|2005-05-0200:00|\n",
      "|2005-05-02|01:00|1.2844|1.2848|1.2839|1.2842|-1.99999999999978E-4|1.2846| 1.285|1.2841|1.2844|-1.99999999999978E-4|2005-05-0201:00|\n",
      "|2005-05-02|02:00|1.2843|1.2854|1.2841|1.2851|7.999999999999119E-4|1.2845|1.2856|1.2843|1.2853|8.000000000001339E-4|2005-05-0202:00|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, col\n",
    "\n",
    "# Creating a new column called Datetime which will be a concatination of the data and time columns\n",
    "df = df.withColumn(\"DateTime\", concat(col(\"Date\"), col(\"Time\")))\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets fix the format and also the type of the datetime column\n",
    "\n",
    "By fixing the format I mean: add a space between the date and the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+-------------------+\n",
      "|      Date| Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|           DateTime|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+-------------------+\n",
      "|2005-05-02|00:00|1.2852|1.2852| 1.284|1.2844|-7.99999999999911...|1.2854|1.2854|1.2842|1.2846|-8.00000000000133...|2005-05-02 00:00:00|\n",
      "|2005-05-02|01:00|1.2844|1.2848|1.2839|1.2842|-1.99999999999978E-4|1.2846| 1.285|1.2841|1.2844|-1.99999999999978E-4|2005-05-02 01:00:00|\n",
      "|2005-05-02|02:00|1.2843|1.2854|1.2841|1.2851|7.999999999999119E-4|1.2845|1.2856|1.2843|1.2853|8.000000000001339E-4|2005-05-02 02:00:00|\n",
      "+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df = df.withColumn(\"DateTime\", to_timestamp(col(\"DateTime\"), \"yyyy-MM-ddHH:mm\"))\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|           DateTime|      Date| Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|2005-05-02 00:00:00|2005-05-02|00:00|1.2852|1.2852| 1.284|1.2844|-7.99999999999911...|1.2854|1.2854|1.2842|1.2846|-8.00000000000133...|\n",
      "|2005-05-02 01:00:00|2005-05-02|01:00|1.2844|1.2848|1.2839|1.2842|-1.99999999999978E-4|1.2846| 1.285|1.2841|1.2844|-1.99999999999978E-4|\n",
      "|2005-05-02 02:00:00|2005-05-02|02:00|1.2843|1.2854|1.2841|1.2851|7.999999999999119E-4|1.2845|1.2856|1.2843|1.2853|8.000000000001339E-4|\n",
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# changing the order of the columns in the PySpark DataFrame\n",
    "df = df.select(\"DateTime\", \"Date\", \"Time\", \"BO\", \"BH\", \"BL\", \"BC\", \"BCh\", \"AO\", \"AH\", \"AL\", \"AC\", \"ACh\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DateTime: timestamp (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- BO: double (nullable = true)\n",
      " |-- BH: double (nullable = true)\n",
      " |-- BL: double (nullable = true)\n",
      " |-- BC: double (nullable = true)\n",
      " |-- BCh: double (nullable = true)\n",
      " |-- AO: double (nullable = true)\n",
      " |-- AH: double (nullable = true)\n",
      " |-- AL: double (nullable = true)\n",
      " |-- AC: double (nullable = true)\n",
      " |-- ACh: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the column types / schema of df\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column and Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "- **Date**: This column contains the date on which the financial data was recorded. Each row in the DataFrame corresponds to a specific date.\n",
    "\n",
    "- **Time**: This column provides the timestamp for each data point, indicating the exact time when the data was captured. This level of granularity allows for tracking changes throughout the trading day.\n",
    "\n",
    "- **Bid Open (BO)**: The \"Bid Open\" column represents the initial bid price at the start of a specific time interval. It's essentially the first price at which traders are willing to buy the EUR/USD currency pair during that interval.\n",
    "\n",
    "- **Bid High (BH)**: This column shows the highest bid price reached during the same time interval. It tells us the peak price that buyers were willing to pay within that period.\n",
    "\n",
    "- **Bid Low (BL)**: Here, we have the lowest bid price observed during the time interval, indicating the lowest price at which traders were willing to buy the currency pair.\n",
    "\n",
    "- **Bid Close (BC)**: The closing bid price signifies the last recorded bid price at the end of the time interval, which can provide insights into how trading sentiment may have evolved during that period.\n",
    "\n",
    "- **Bid Change (BCh)**: This column seems to represent the change in bid price during the interval, possibly calculated as the difference between the opening and closing bid prices. It helps track price movements.\n",
    "\n",
    "- **Ask Open (AO)**: Similar to the bid open, this is the initial asking price for the EUR/USD currency pair at the beginning of the time interval. It represents the first price at which sellers are willing to sell.\n",
    "\n",
    "- **Ask High (AH)**: The highest asking price during the time interval is found here. It indicates the peak price sellers were requesting for the currency pair.\n",
    "\n",
    "- **Ask Low (AL)**: The lowest asking price observed during the interval is listed in this column, representing the lowest price sellers were willing to accept.\n",
    "\n",
    "- **Ask Close (AC)**: The closing asking price signifies the last recorded asking price at the end of the time interval. Like the closing bid price, it offers insights into potential shifts in market sentiment.\n",
    "\n",
    "- **Ask Change (ACh)**: Similar to the bid change, this column appears to represent the change in asking price during the interval, possibly calculated as the difference between the opening and closing asking prices. It helps monitor price fluctuations from the perspective of sellers.\n",
    "\n",
    "This dataset provides a comprehensive view of trading activity for the EUR/USD currency pair, including open, high, low, and close prices for both bid and ask prices. It's invaluable for financial analysis and can be used to analyze market trends, make trading decisions, and gain insights into the behavior of currency pairs in the foreign exchange market.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the dimensions of our Pyspark SQL Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|           DateTime|      Date| Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|2005-05-02 00:00:00|2005-05-02|00:00|1.2852|1.2852| 1.284|1.2844|-7.99999999999911...|1.2854|1.2854|1.2842|1.2846|-8.00000000000133...|\n",
      "|2005-05-02 01:00:00|2005-05-02|01:00|1.2844|1.2848|1.2839|1.2842|-1.99999999999978E-4|1.2846| 1.285|1.2841|1.2844|-1.99999999999978E-4|\n",
      "|2005-05-02 02:00:00|2005-05-02|02:00|1.2843|1.2854|1.2841|1.2851|7.999999999999119E-4|1.2845|1.2856|1.2843|1.2853|8.000000000001339E-4|\n",
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 93084\n",
      "Number of Columns: 13\n"
     ]
    }
   ],
   "source": [
    "# and 13 columns ( 1 that I introduced - DateTime )\n",
    "# we have 93084 rows in our dataframe\n",
    "print(f\"Number of Rows: {df.count()}\")\n",
    "print(f\"Number of Columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93084"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.count()\n",
    "# we have the same number of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "Apparently we have 6 days of week worth of data in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2005-05-02|\n",
      "|2005-05-03|\n",
      "|2005-05-04|\n",
      "|2005-05-05|\n",
      "|2005-05-06|\n",
      "|2005-05-08|\n",
      "|2005-05-09|\n",
      "|2005-05-10|\n",
      "|2005-05-11|\n",
      "|2005-05-12|\n",
      "|2005-05-13|\n",
      "|2005-05-15|\n",
      "|2005-05-16|\n",
      "|2005-05-17|\n",
      "|2005-05-18|\n",
      "|2005-05-19|\n",
      "|2005-05-20|\n",
      "|2005-05-22|\n",
      "|2005-05-23|\n",
      "|2005-05-24|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "sorted_dates = df_pyspark.select('Date').distinct().orderBy(col('Date'))\n",
    "\n",
    "sorted_dates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out why we have data for 6 days a week instead of 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forex (foreign exchange) markets are generally open and available for trading 24 hours a day, five days a week. The forex market operates globally and is divided into different trading sessions, which include:\n",
    "\n",
    "Sydney Session: This session starts at 10:00 PM GMT and ends at 7:00 AM GMT. It is the first major session to open.\n",
    "\n",
    "Tokyo Session: Following the Sydney session, the Tokyo session begins at 11:00 PM GMT and ends at 8:00 AM GMT.\n",
    "\n",
    "London Session: The London session is one of the most significant and active trading sessions. It starts at 8:00 AM GMT and ends at 4:00 PM GMT.\n",
    "\n",
    "New York Session: The New York session overlaps with the London session and is open from 1:00 PM GMT to 10:00 PM GMT. This overlap period is typically the busiest time for forex trading.\n",
    "\n",
    "During these trading sessions, currency pairs like EUR/USD (Euro/US Dollar) are actively traded. However, it's important to note that the forex market is closed on weekends (from Friday evening until Sunday evening GMT) and during certain holidays when major financial centers are closed.\n",
    "\n",
    "So apparently forex trading is generally available every day from Sunday evening GMT to Friday evening GMT,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|           DateTime|      Date| Time|    BO|    BH|    BL|    BC|                 BCh|    AO|    AH|    AL|    AC|                 ACh|\n",
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "|2005-05-02 00:00:00|2005-05-02|00:00|1.2852|1.2852| 1.284|1.2844|-7.99999999999911...|1.2854|1.2854|1.2842|1.2846|-8.00000000000133...|\n",
      "|2005-05-02 01:00:00|2005-05-02|01:00|1.2844|1.2848|1.2839|1.2842|-1.99999999999978E-4|1.2846| 1.285|1.2841|1.2844|-1.99999999999978E-4|\n",
      "|2005-05-02 02:00:00|2005-05-02|02:00|1.2843|1.2854|1.2841|1.2851|7.999999999999119E-4|1.2845|1.2856|1.2843|1.2853|8.000000000001339E-4|\n",
      "+-------------------+----------+-----+------+------+------+------+--------------------+------+------+------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+-------+-------+-------+-------+--------------------+------------------+-------+-------+-------+--------------------+\n",
      "|           DateTime|      Date| Time|     BO|     BH|     BL|     BC|                 BCh|                AO|     AH|     AL|     AC|                 ACh|\n",
      "+-------------------+----------+-----+-------+-------+-------+-------+--------------------+------------------+-------+-------+-------+--------------------+\n",
      "|2019-09-15 21:00:00|2019-09-15|21:00|1.10727|1.10743|1.10678|1.10712|-1.49999999999872...|           1.10768| 1.1079| 1.1072|1.10749|-1.90000000000134...|\n",
      "|2019-09-15 22:00:00|2019-09-15|22:00|1.10712|1.10794|1.10665|1.10785|7.299999999998974E-4|           1.10755|1.10811|1.10734|1.10801|4.599999999999049E-4|\n",
      "|2019-09-15 23:00:00|2019-09-15|23:00|1.10786|1.10858|1.10783|1.10825|3.899999999998904E-4|1.1080299999999998|1.10871|1.10798|1.10839| 3.60000000000138E-4|\n",
      "+-------------------+----------+-----+-------+-------+-------+-------+--------------------+------------------+-------+-------+-------+--------------------+\n",
      "\n",
      "The selected date (2019-09-15) was a Sunday.\n",
      "Hour range for 2019-09-15:         21:00      to      23:00\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "selected_date = \"2019-09-15\"\n",
    "\n",
    "# Filter the DataFrame to select data for the specified date (we picked a random Sunday)\n",
    "filtered_df = df.filter(df['Date'] == selected_date)\n",
    "\n",
    "# Show the filtered data\n",
    "filtered_df.show()\n",
    "\n",
    "hour_range = filtered_df.select(min('Time'), max('Time')).first()\n",
    "\n",
    "# Extract the minimum and maximum values\n",
    "min_hour = hour_range[0]\n",
    "max_hour = hour_range[1]\n",
    "\n",
    "day_of_week = filtered_df.select(date_format('Date', 'EEEE').alias('DayOfWeek')).first()\n",
    "\n",
    "# Extract the day of the week\n",
    "day_of_week = day_of_week['DayOfWeek']\n",
    "\n",
    "# Display the day of the week\n",
    "print(f\"The selected date ({selected_date}) was a {day_of_week}.\")\n",
    "\n",
    "# Display the hour range\n",
    "print(f\"Hour range for {selected_date}:         {min_hour}      to      {max_hour}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+------------------+------------------+------------------+------------------+--------------------+------------------+-------+------------------+------------------+--------------------+\n",
      "|           DateTime|      Date| Time|                BO|                BH|                BL|                BC|                 BCh|                AO|     AH|                AL|                AC|                 ACh|\n",
      "+-------------------+----------+-----+------------------+------------------+------------------+------------------+--------------------+------------------+-------+------------------+------------------+--------------------+\n",
      "|2019-09-13 00:00:00|2019-09-13|00:00|1.1055700000000002|           1.10643|1.1055700000000002|           1.10636|7.899999999998464E-4|           1.10573|1.10656|           1.10572|           1.10649|7.600000000000938E-4|\n",
      "|2019-09-13 01:00:00|2019-09-13|01:00|           1.10635|           1.10658|           1.10622|           1.10641|6.000000000017103...|           1.10648| 1.1067|           1.10636|           1.10654|5.999999999994897...|\n",
      "|2019-09-13 02:00:00|2019-09-13|02:00|            1.1064|           1.10659|           1.10633|1.1065200000000002|  1.2000000000012E-4|           1.10653|1.10672|           1.10647|           1.10665|1.199999999998979...|\n",
      "|2019-09-13 03:00:00|2019-09-13|03:00|           1.10653|           1.10687|            1.1065|           1.10686|3.299999999999414...|           1.10666|  1.107|           1.10663|             1.107|3.400000000000069...|\n",
      "|2019-09-13 04:00:00|2019-09-13|04:00|           1.10685|           1.10727|           1.10663|           1.10689|4.000000000004000...|           1.10699|1.10741|           1.10676|           1.10702|3.000000000019653...|\n",
      "|2019-09-13 05:00:00|2019-09-13|05:00|           1.10687|           1.10725|           1.10686|           1.10703|1.599999999999379...|           1.10701|1.10739|           1.10698|           1.10717|1.599999999999379...|\n",
      "|2019-09-13 06:00:00|2019-09-13|06:00|           1.10705|           1.10961|           1.10673|           1.10952|0.002469999999999972|1.1071799999999998|1.10973|           1.10686|           1.10965|0.002470000000000...|\n",
      "|2019-09-13 07:00:00|2019-09-13|07:00|           1.10953|            1.1109|           1.10912|           1.10934|-1.89999999999912...|           1.10966|1.11102|           1.10925|           1.10945|-2.10000000000043...|\n",
      "|2019-09-13 08:00:00|2019-09-13|08:00|           1.10932|           1.10971|           1.10765|           1.10917|-1.50000000000094...|           1.10945|1.10983|           1.10781|           1.10932|-1.29999999999963...|\n",
      "|2019-09-13 09:00:00|2019-09-13|09:00|           1.10919|           1.11038|            1.1089|           1.11002|8.300000000001084E-4|           1.10932| 1.1105|           1.10904|           1.11012|7.999999999999119E-4|\n",
      "|2019-09-13 10:00:00|2019-09-13|10:00|           1.11002|           1.11056|           1.10899|           1.10909|-9.30000000000097...|           1.11014|1.11068|           1.10912|           1.10921|-9.29999999999875...|\n",
      "|2019-09-13 11:00:00|2019-09-13|11:00|1.1090799999999998|           1.11061|           1.10904|           1.11022|0.001140000000000...|            1.1092|1.11073|           1.10917|           1.11034|0.001139999999999...|\n",
      "|2019-09-13 12:00:00|2019-09-13|12:00|           1.11021|           1.11044|           1.10785|            1.1087|-0.00151000000000...|1.1103299999999998|1.11057|           1.10798|           1.10883|-0.00149999999999...|\n",
      "|2019-09-13 13:00:00|2019-09-13|13:00|           1.10869|           1.10876|            1.1065|           1.10682|-0.00186999999999...|           1.10882|1.10889|1.1066200000000002|           1.10697|-0.00185000000000...|\n",
      "|2019-09-13 14:00:00|2019-09-13|14:00|           1.10679|1.1085200000000002|1.1060299999999998|           1.10799| 0.00120000000000009|           1.10696|1.10864|           1.10615|           1.10811|0.001149999999999...|\n",
      "|2019-09-13 15:00:00|2019-09-13|15:00|             1.108|            1.1085|           1.10689|           1.10735|-6.49999999999817...|           1.10812|1.10861|           1.10701|1.1074700000000002|-6.49999999999817...|\n",
      "|2019-09-13 16:00:00|2019-09-13|16:00|           1.10733|           1.10761|1.1066200000000002|           1.10754|2.100000000000434...|1.1074700000000002|1.10774|           1.10674|           1.10765|1.799999999998469...|\n",
      "|2019-09-13 17:00:00|2019-09-13|17:00|           1.10753|           1.10793|             1.107|           1.10759|6.000000000017103...|           1.10764|1.10806|           1.10712|           1.10771|7.000000000001451E-5|\n",
      "|2019-09-13 18:00:00|2019-09-13|18:00|           1.10758|1.1076700000000002|1.1066200000000002|           1.10665|-9.30000000000097...|            1.1077|1.10779|           1.10676|           1.10678|-9.20000000000032E-4|\n",
      "|2019-09-13 19:00:00|2019-09-13|19:00|           1.10666|1.1076700000000002|           1.10666|           1.10759|9.300000000000974E-4|           1.10679| 1.1078|           1.10679|           1.10772|9.300000000000974E-4|\n",
      "+-------------------+----------+-----+------------------+------------------+------------------+------------------+--------------------+------------------+-------+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "The selected date (2019-09-13) was a Friday.\n",
      "Hour range for 2019-09-13:         00:00      to      20:00\n"
     ]
    }
   ],
   "source": [
    "selected_date = \"2019-09-13\"\n",
    "\n",
    "\n",
    "# Filter the DataFrame to select data for the specified date (we picked a random Sunday)\n",
    "filtered_df = df.filter(df['Date'] == selected_date)\n",
    "\n",
    "# Show the filtered data\n",
    "filtered_df.show()\n",
    "\n",
    "hour_range = filtered_df.select(min('Time'), max('Time')).first()\n",
    "\n",
    "# Extract the minimum and maximum values\n",
    "min_hour = hour_range[0]\n",
    "max_hour = hour_range[1]\n",
    "\n",
    "day_of_week = filtered_df.select(date_format('Date', 'EEEE').alias('DayOfWeek')).first()\n",
    "\n",
    "# Extract the day of the week\n",
    "day_of_week = day_of_week['DayOfWeek']\n",
    "\n",
    "# Display the day of the week\n",
    "print(f\"The selected date ({selected_date}) was a {day_of_week}.\")\n",
    "\n",
    "# Display the hour range\n",
    "print(f\"Hour range for {selected_date}:         {min_hour}      to      {max_hour}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+------------------+------------------+-------+------------------+--------------------+------------------+------------------+------------------+------------------+--------------------+\n",
      "|           DateTime|      Date| Time|                BO|                BH|     BL|                BC|                 BCh|                AO|                AH|                AL|                AC|                 ACh|\n",
      "+-------------------+----------+-----+------------------+------------------+-------+------------------+--------------------+------------------+------------------+------------------+------------------+--------------------+\n",
      "|2019-09-12 00:00:00|2019-09-12|00:00|           1.10131|1.1013700000000002|1.10074|             1.101|-3.10000000000032...|           1.10144|           1.10149|           1.10086|           1.10114|-2.99999999999966...|\n",
      "|2019-09-12 01:00:00|2019-09-12|01:00|           1.10102|1.1014700000000002|1.10102|           1.10131|2.899999999999014E-4|           1.10114|            1.1016|           1.10114|           1.10143|2.899999999999014E-4|\n",
      "|2019-09-12 02:00:00|2019-09-12|02:00|           1.10128|           1.10129|1.10081|            1.1009|-3.80000000000046...|           1.10142|           1.10142|           1.10095|           1.10104|-3.80000000000046...|\n",
      "|2019-09-12 03:00:00|2019-09-12|03:00|           1.10092|            1.1013|1.10092|           1.10124| 3.19999999999876E-4|           1.10105|           1.10143|           1.10105|           1.10138|3.299999999999414...|\n",
      "|2019-09-12 04:00:00|2019-09-12|04:00|           1.10123|           1.10142|1.10113|           1.10135|  1.2000000000012E-4|1.1013700000000002|           1.10155|           1.10126|1.1014700000000002|  9.9999999999989E-5|\n",
      "|2019-09-12 05:00:00|2019-09-12|05:00|           1.10133|           1.10144|1.10115|           1.10129|-4.00000000000400...|           1.10146|           1.10156|           1.10127|           1.10142|-4.00000000000400...|\n",
      "|2019-09-12 06:00:00|2019-09-12|06:00|           1.10129|           1.10143|1.10051|1.1013700000000002|8.000000000030205E-5|           1.10142|           1.10155|1.1006200000000002|            1.1015|7.999999999985796E-5|\n",
      "|2019-09-12 07:00:00|2019-09-12|07:00|1.1013700000000002|           1.10184| 1.1011|           1.10164|2.699999999997704E-4|           1.10148|           1.10197|           1.10123|           1.10177|2.900000000001235E-4|\n",
      "|2019-09-12 08:00:00|2019-09-12|08:00|           1.10166|           1.10319|1.10151|           1.10264|9.799999999999807E-4|           1.10177|           1.10332|           1.10163|           1.10278|0.001009999999999...|\n",
      "|2019-09-12 09:00:00|2019-09-12|09:00|           1.10263|           1.10309|1.10244|            1.1029|2.699999999999924...|           1.10277|           1.10322|           1.10258|           1.10302|2.500000000000835E-4|\n",
      "|2019-09-12 10:00:00|2019-09-12|10:00|            1.1029|           1.10312|1.10192|           1.10233|-5.69999999999959...|           1.10304|           1.10323|1.1020299999999998|           1.10247|-5.69999999999959...|\n",
      "|2019-09-12 11:00:00|2019-09-12|11:00|           1.10236|           1.10663|1.09604|1.0965200000000002|-0.00583999999999...|           1.10247|           1.10743|           1.09619|           1.09666|-0.00581000000000...|\n",
      "|2019-09-12 12:00:00|2019-09-12|12:00|           1.09651|           1.09728|1.09262|           1.09454|-0.00197000000000...|           1.09665|           1.09741|           1.09275|           1.09467|-0.00197999999999...|\n",
      "|2019-09-12 13:00:00|2019-09-12|13:00|           1.09451|            1.1046|1.09397|1.1034700000000002|0.008960000000000079|           1.09466|           1.10477|           1.09411|            1.1036|0.008939999999999948|\n",
      "|2019-09-12 14:00:00|2019-09-12|14:00|           1.10348|           1.10686|1.10279|            1.1029|-5.80000000000024...|           1.10361|           1.10699|           1.10293|           1.10303|-5.80000000000024...|\n",
      "|2019-09-12 15:00:00|2019-09-12|15:00|           1.10292|           1.10789|1.10255|            1.1073|0.004379999999999...|           1.10304|1.1080299999999998|           1.10268|           1.10741|0.004369999999999985|\n",
      "|2019-09-12 16:00:00|2019-09-12|16:00|           1.10729|           1.10739| 1.1061|           1.10672|-5.69999999999737...|            1.1074|           1.10751|1.1062299999999998|           1.10685|-5.50000000000050...|\n",
      "|2019-09-12 17:00:00|2019-09-12|17:00|           1.10671|           1.10866|1.10654|           1.10665|-6.00000000001710...|           1.10683|            1.1088|           1.10666|           1.10677|-5.99999999999489...|\n",
      "|2019-09-12 18:00:00|2019-09-12|18:00|           1.10666|           1.10748|1.10632|           1.10734|6.800000000000138E-4|           1.10678|           1.10759|           1.10643|           1.10745|6.700000000001705E-4|\n",
      "|2019-09-12 19:00:00|2019-09-12|19:00|           1.10734|           1.10734|1.10618|           1.10645|-8.90000000000057...|1.1074700000000002|1.1074700000000002|            1.1063|           1.10655|-9.20000000000254E-4|\n",
      "+-------------------+----------+-----+------------------+------------------+-------+------------------+--------------------+------------------+------------------+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "The selected date (2019-09-12) was a Thursday.\n",
      "Hour range for 2019-09-12:         00:00      to      23:00\n"
     ]
    }
   ],
   "source": [
    "selected_date = \"2019-09-12\"\n",
    "\n",
    "# Filter the DataFrame to select data for the specified date (we picked a random Sunday)\n",
    "filtered_df = df.filter(df['Date'] == selected_date)\n",
    "\n",
    "# Show the filtered data\n",
    "filtered_df.show()\n",
    "\n",
    "hour_range = filtered_df.select(min('Time'), max('Time')).first()\n",
    "\n",
    "# Extract the minimum and maximum values\n",
    "min_hour = hour_range[0]\n",
    "max_hour = hour_range[1]\n",
    "\n",
    "day_of_week = filtered_df.select(date_format('Date', 'EEEE').alias('DayOfWeek')).first()\n",
    "\n",
    "# Extract the day of the week\n",
    "day_of_week = day_of_week['DayOfWeek']\n",
    "\n",
    "# Display the day of the week\n",
    "print(f\"The selected date ({selected_date}) was a {day_of_week}.\")\n",
    "\n",
    "# Display the hour range\n",
    "print(f\"Hour range for {selected_date}:         {min_hour}      to      {max_hour}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So apparently the Data Range for each week is from :\n",
    "\n",
    "Sunday              from 21:00 to 23:00\n",
    "\n",
    "Monday - Thursday   from 00:00 to 23:00\n",
    "\n",
    "Friday              from 00:00 to 21:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the summary statistics generated in the previous step in a tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+--------------------+\n",
      "|summary|                 BO|                BH|                BL|                BC|                 BCh|                 AO|                AH|                AC|                 ACh|\n",
      "+-------+-------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+--------------------+\n",
      "|  count|              93084|             93084|             93084|             93084|               93084|              93084|             93084|             93084|               93084|\n",
      "|   mean| 1.2657340146534282|1.2667553338919628|1.2647362568217932| 1.265733121803963|-8.92849469295910...| 1.2659002723346635|1.2669171722315304|1.2658992623866556|-1.00994800395440...|\n",
      "| stddev|0.12689441842290416|0.1270596992463785| 0.126722172862712|0.1268923255197565|0.001544142859950...|0.12689052960293262|0.1270531364216383|0.1268911503779533|0.001545720757460...|\n",
      "|    min| 1.0355299999999998|1.0369700000000002|           1.03395|           1.03555|-0.02505000000000...|            1.03566|            1.0371|           1.03568|-0.02495000000000...|\n",
      "|    max|            1.60131|           1.60384|            1.5992|           1.60139|0.030219999999999917|             1.6014|           1.60393|           1.60148|0.030429999999999964|\n",
      "+-------+-------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"BO\", \"BH\", \"BL\", \"BC\", \"BCh\", \"AO\", \"AH\", \"AC\", \"ACh\").describe().show()\n",
    "# show summary statistics only for the numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find the null values in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+---+---+---+---+---+---+---+---+---+---+\n",
      "|DateTime|Date|Time| BO| BH| BL| BC|BCh| AO| AH| AL| AC|ACh|\n",
      "+--------+----+----+---+---+---+---+---+---+---+---+---+---+\n",
      "|       0|   0|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+--------+----+----+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts = df.select([sum(col(column).isNull().cast(\"int\")).alias(column) for column in df.columns])\n",
    "\n",
    "null_counts.show()\n",
    "# apparently we have no nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we had nulls we could have used this statement and set the parameter how  to all \n",
    "# to drops rows that have the entire row has NULL values it will be dropped\n",
    "df = df.na.drop(how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93084"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we drop the row if it doesn't have 2 NON NULL values in the cells of the row\n",
    "df = df.na.drop(how=\"all\", thresh=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93084"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's nice! we have 0 null values in our DataFrame!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check if we have 0 values in our pyspark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+\n",
      "| BO| BH| BL| BC|BCh| AO| AH| AL| AC|ACh|\n",
      "+---+---+---+---+---+---+---+---+---+---+\n",
      "|  0|  0|  0|  0|692|  0|  0|  0|  0|674|\n",
      "+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# List of numeric column names\n",
    "numeric_columns = [column for column in df.columns if df.schema[column].dataType in {DoubleType(), IntegerType()}]\n",
    "\n",
    "# Check for 0 values in numeric columns\n",
    "zero_counts = df.select([sum((col(column) == 0).cast(\"int\")).alias(column) for column in numeric_columns])\n",
    "\n",
    "zero_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternative way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+\n",
      "| BO| BH| BL| BC|BCh| AO| AH| AL| AC|ACh|\n",
      "+---+---+---+---+---+---+---+---+---+---+\n",
      "|  0|  0|  0|  0|692|  0|  0|  0|  0|674|\n",
      "+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_columns = ['BO', 'BH', 'BL', 'BC', 'BCh', 'AO', 'AH', 'AL', 'AC', 'ACh']\n",
    "\n",
    "# Check for 0 values in the numeric columns\n",
    "zero_counts = df.select([F.sum(F.when(df[column] == 0, 1).otherwise(0)).alias(column) for column in numeric_columns])\n",
    "\n",
    "# Show the result\n",
    "zero_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----+------------------+------------------+------------------+------------------+---+-------+-------+------------------+-------+---+\n",
      "|           DateTime|      Date| Time|                BO|                BH|                BL|                BC|BCh|     AO|     AH|                AL|     AC|ACh|\n",
      "+-------------------+----------+-----+------------------+------------------+------------------+------------------+---+-------+-------+------------------+-------+---+\n",
      "|2005-05-02 03:00:00|2005-05-02|03:00|            1.2851|            1.2859|             1.285|            1.2851|0.0| 1.2853| 1.2861|            1.2852| 1.2853|0.0|\n",
      "|2005-05-02 05:00:00|2005-05-02|05:00|            1.2854|            1.2858|            1.2853|            1.2854|0.0| 1.2856|  1.286|            1.2855| 1.2856|0.0|\n",
      "|2005-05-03 02:00:00|2005-05-03|02:00|            1.2833|            1.2836|            1.2831|            1.2833|0.0| 1.2835| 1.2838|            1.2833| 1.2835|0.0|\n",
      "|2005-05-04 12:00:00|2005-05-04|12:00|            1.2951|             1.296|           1.29455|            1.2951|0.0|1.29525|1.29615|            1.2947|1.29525|0.0|\n",
      "|2005-05-05 20:00:00|2005-05-05|20:00|1.2956299999999998|1.2957299999999998|           1.29513|1.2956299999999998|0.0|1.29578|1.29588|           1.29528|1.29578|0.0|\n",
      "|2005-05-06 09:00:00|2005-05-06|09:00|            1.2952|            1.2953|           1.29455|            1.2952|0.0|1.29535|1.29545|            1.2947|1.29535|0.0|\n",
      "|2005-05-09 20:00:00|2005-05-09|20:00|            1.2841|            1.2846|            1.2841|            1.2841|0.0|1.28425|1.28475|           1.28425|1.28425|0.0|\n",
      "|2005-05-12 18:00:00|2005-05-12|18:00|           1.27053|           1.27123|1.2694299999999998|           1.27053|0.0|1.27068|1.27138|           1.26958|1.27068|0.0|\n",
      "|2005-05-13 17:00:00|2005-05-13|17:00|           1.26263|1.2633299999999998|           1.26233|           1.26263|0.0|1.26278|1.26348|1.2624799999999998|1.26278|0.0|\n",
      "|2005-05-16 01:00:00|2005-05-16|01:00|            1.2605|            1.2606|            1.2601|            1.2605|0.0|1.26065|1.26075|           1.26025|1.26065|0.0|\n",
      "+-------------------+----------+-----+------------------+------------------+------------------+------------------+---+-------+-------+------------------+-------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# displaying the rows that have 0 values in BCh or Ach columns\n",
    "filtered_df = df.filter((col('BCh') == 0) | (col('ACh') == 0))\n",
    "\n",
    "filtered_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on 0 Values in Bid Change (BCh) and Ask Change (ACh) Columns\n",
    "\n",
    "In the Forex market, the Bid Open (BO) and Bid Close (BC) columns, as well as the Ask Open (AO) and Ask Close (AC) columns, represent the opening and closing prices for bid and ask orders, respectively. In instances where these values are equal, it results in a Bid Change (BCh) and Ask Change (ACh) of 0.\n",
    "\n",
    "This occurrence indicates a state of price stability where there is no change in the bid or ask prices throughout the time interval. In financial terms, this reflects a period where the opening and closing prices remain constant, signaling a lack of fluctuation in the market during that specific time frame.\n",
    "\n",
    "While 0 values in BCh and ACh might seem unusual, they are a valid representation of price stability, and it is a common scenario in financial data analysis. Therefore, for the purpose of this analysis, these 0 values will not be modified, as they accurately capture the absence of price changes during certain intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check if we have empty strings in our pyspark dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+---+---+---+---+---+---+---+---+---+---+\n",
      "|DateTime|Date|Time| BO| BH| BL| BC|BCh| AO| AH| AL| AC|ACh|\n",
      "+--------+----+----+---+---+---+---+---+---+---+---+---+---+\n",
      "|       0|   0|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+--------+----+----+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Selecting all columns from the DataFrame\n",
    "columns = df.columns\n",
    "\n",
    "# Checking for empty strings in each column\n",
    "empty_string_counts = df.select([sum(when(col(column) == \"\", 1).otherwise(0)).alias(column) for column in columns])\n",
    "\n",
    "# Displaying the result\n",
    "empty_string_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great we do not have empty strings in our dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Column Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.withColumn(\"PriceRange\", col(\"BH\") - col(\"BL\"))\n",
    "# # we can also selected multiple columns\n",
    "# df.select(['Datetime','Date','Time','BO']).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating New Columns and Explain Why\n",
    "# Let's assume you want to calculate the daily price range as a new column\n",
    "# Price range = High Price (BH) - Low Price (BL)\n",
    "\n",
    "# df = df.withColumn(\"PriceRange\", col(\"BH\") - col(\"BL\"))\n",
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
